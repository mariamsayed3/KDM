# spatial_transcriptomics_kd.yml
# Configuration for knowledge distillation (teacher -> student)

name: 'spatial_transcriptomics_kd'
seed: 42

# Global parameters
dataset: 'spatial_transcriptomics'
gpu_id: [0]

train_params:
  dataset: 'spatial_transcriptomics'
  dataset_dir: 'C:/Users/maria/Documents/Masters_24/New_Models/KDM_Model/KDM/bioDataset3'  # Update this path
  batch_size: 16
  n_epochs: 100
  num_workers: 4
  save_dir: ''  # Will be auto-generated with timestamp
  early_stop: 25
  ignore_index: 0

# Teacher model configuration (pre-trained)
teacher_params:
  name: 'SGR_Net'  # Same as teacher training
  n_bands: 136
  classes: null  # Auto-detect classes
  n_heads: 5
  nf_enc: [64, 128, 256, 512, 1024]
  nf_dec: [64, 32, 32, 16, 16, 8]
  do_batchnorm: true
  encoder_name: 'resnet50'
  
  # Path to pre-trained teacher model
  pretrained_file: 'experiments/spatial_transcriptomics_teacher-[timestamp]/best_model.pth'
  
  ignore_index: 0

# Student model configuration (to be trained)
student_params:
  name: 'HSINet'  # Smaller student model
  n_bands: 136
  classes: null  # Auto-detect classes (same as teacher)
  nf_enc: [32, 64, 128, 256]  # Smaller than teacher
  nf_dec: [32, 32, 16, 16, 8, 8]
  do_batchnorm: true
  
  # Knowledge distillation weights
  feat_weight: 0.5     # Feature distillation weight
  resp_weight: 0.5     # Response distillation weight
  temperature: 4.0     # Softmax temperature for KD
  
  ignore_index: 0

optimizer:
  type: 'Adam'
  args:
    lr: 0.0001
    weight_decay: 0.00001

# Knowledge distillation loss
loss: 'KDMLoss'

# Metrics with auto-class detection  
metric: 'MeanIoU'

# Resume training if needed
resume: false

tags: 
  - 'spatial_transcriptomics'
  - 'knowledge_distillation'
  - 'student'